from stratadl.core.provider.base import BaseProvider
from stratadl.core.provider.stream import OllamaStreamResult, OllamaDownloadResult
import requests
import json
from typing import List
import sys


class OllamaProvider(BaseProvider):

    def __init__(self, config=None):
        super().__init__(config)
        self.api_url = self.config.get("api_url", "http://localhost:11434")
        self.model = self.config.get("model", "llama3.2")

    def set_model(self, model_name:str):
        """
        Permet de setter un mod√®le
        """

        self.model = model_name

    def generate(self, prompt: str, stream: bool = False, **kwargs):
        """
        G√©n√®re une r√©ponse depuis Ollama.

        Args:
            prompt: Le texte d'entr√©e
            stream: Si True, affiche en temps r√©el et retourne un g√©n√©rateur
            **kwargs: Arguments suppl√©mentaires pour l'API Ollama

        Returns:
            Si stream=False: str (texte complet)
            Si stream=True: g√©n√©rateur qui yield chaque chunk
        """
        try:
            payload = {
                "model": self.model,
                "prompt": prompt,
                "stream": stream,
                **kwargs
            }

            response = requests.post(
                f"{self.api_url}/api/generate",
                json=payload,
                stream=stream
            )
            response.raise_for_status()

            if stream:
                return self._handle_stream(response)
            else:
                return self._handle_non_stream(response)

        except requests.exceptions.HTTPError as e:
            print("HTTP error:", e)
            if e.response is not None:
                print("Response content:", e.response.text)
            return None
        except requests.exceptions.RequestException as e:
            print("Request failed:", e)
            return None

    def chat(self, messages:List[dict], stream:bool = False, **kwargs):
        """
        G√©n√®re le message suivant √† une conversation
        """

        try:
            payload = {
                "model": self.model,
                "messages": messages,
                "stream": stream,
                **kwargs
            }

            response = requests.post(
                f"{self.api_url}/api/chat",
                json=payload,
                stream=stream
            )
            response.raise_for_status()

            if stream:
                return self._handle_stream(response)
            else:
                return self._handle_non_stream(response)

        except requests.exceptions.HTTPError as e:
            print("HTTP error:", e)
            if e.response is not None:
                print("Response content:", e.response.text)
            return None
        except requests.exceptions.RequestException as e:
            print("Request failed:", e)
            return None

    def _handle_stream(self, response):
        def stream_generator():
            for line in response.iter_lines():
                if line:
                    chunk = json.loads(line.decode('utf-8'))
                    yield chunk

                    if chunk.get('done', False):
                        break

        return OllamaStreamResult(stream_generator())

    def _handle_non_stream(self, response):
        """G√®re la r√©ponse non-stream√©e (JSON unique)"""

        try:
            data = json.loads(response.text)
        except json.JSONDecodeError as e:
            print("Debug: JSON decode error:", e)
            print("Debug: Raw response:", response.text)
            return ""

        # Acc√®s direct au texte dans le champ message.content
        message = data.get("message", {})
        content = message.get("content", "")

        return content.strip()

    def embed(self, text: str, **kwargs) -> list[float]:
        try:
            payload = {"model": self.model, "input": text, **kwargs}
            response = requests.post(f"{self.api_url}/api/embed", json=payload)
            response.raise_for_status()
            return response.json()

        except requests.exceptions.HTTPError as e:
            print("HTTP error:", e)
            if e.response is not None:
                print("Response content:", e.response.text)
            return None
        except requests.exceptions.RequestException as e:
            print("Request failed:", e)
            return None

    def list_models(self):
        """
            Permet de retourner la liste des mod√®les t√©l√©charger
        """

        try:

            response = requests.get(f"{self.api_url}/api/tags")
            response.raise_for_status()
            return response.json()

        except requests.exceptions.HTTPError as e:
            print("HTTP error:", e)
            if e.response is not None:
                print("Response content:", e.response.text)
            return None
        except requests.exceptions.RequestException as e:
            print("Request failed:", e)
            return None

    def model_info(self, verbose:bool = False):
        """
            Permet de retourner les informations du mod√®le en cours
        """
        try:
            payload = {"model": self.model, "verbose": verbose}
            response = requests.post(f"{self.api_url}/api/show", json=payload)
            response.raise_for_status()
            return response.json()

        except requests.exceptions.HTTPError as e:
            print("HTTP error:", e)
            if e.response is not None:
                print("Response content:", e.response.text)
            return None
        except requests.exceptions.RequestException as e:
            print("Request failed:", e)
            return None

    def pull_model(self, model: str, stream: bool = False, insecure: bool = False, **kwargs):
        """
        T√©l√©charge un mod√®le depuis la biblioth√®que Ollama

        Args:
            model: nom du mod√®le √† t√©l√©charger
            stream: si False, retourne la r√©ponse finale uniquement
            insecure: permet les connexions non s√©curis√©es
            **kwargs: arguments suppl√©mentaires

        Returns:
            Si stream=False: dict avec le statut final
            Si stream=True: OllamaDownloadResult (g√©n√©rateur)
        """
        try:
            payload = {
                "model": model,
                "stream": stream,
                "insecure": insecure,
                **kwargs
            }

            response = requests.post(
                f"{self.api_url}/api/pull",
                json=payload,
                stream=stream
            )
            response.raise_for_status()

            if stream:
                return self._handle_pull_stream(response)
            else:
                return self._handle_pull_non_stream(response)

        except requests.exceptions.HTTPError as e:
            print("HTTP error:", e)
            if e.response is not None:
                print("Response content:", e.response.text)
            return None
        except requests.exceptions.RequestException as e:
            print("Request failed:", e)
            return None

    def _handle_pull_stream(self, response):
        """G√®re le t√©l√©chargement en mode streaming"""
        def download_generator():
            for line in response.iter_lines():
                if line:
                    chunk = json.loads(line.decode('utf-8'))
                    yield chunk

                    if chunk.get('status') == 'success':
                        break

        return OllamaDownloadResult(download_generator())

    def download(self, model_name:str):
        """
            Permet de lancer le t√©l√©chargement tout en affichant la progression
        """
        result = self.pull_model(model_name, stream=True)

        for chunk in result:
            status = chunk.get('status', '')

            if status == 'pulling manifest':
                print("üì• R√©cup√©ration du manifest...")

            elif "pulling" in status:
                digest = chunk.get('digest', '')[:12]
                completed = chunk.get('completed', 0)
                total = chunk.get('total', 1)
                percentage = (completed / total) * 100

                # Barre de progression
                bar_length = 40
                filled = int(bar_length * completed / total)
                bar = '‚ñà' * filled + '‚ñë' * (bar_length - filled)

                mb_completed = completed / (1024 * 1024)
                mb_total = total / (1024 * 1024)

                sys.stdout.write(f'\r{digest}: [{bar}] {percentage:.1f}% ({mb_completed:.1f}/{mb_total:.1f} MB)')
                sys.stdout.flush()

            elif status == 'verifying sha256 digest':
                print("\n‚úì V√©rification de l'int√©grit√©...")

            elif status == 'writing manifest':
                print("‚úì √âcriture du manifest...")

            elif status == 'removing any unused layers':
                print("‚úì Nettoyage des couches inutilis√©es...")

            elif status == 'success':
                print("‚úÖ T√©l√©chargement termin√© avec succ√®s!")

        # Statistiques finales
        stats = result.get_stats()
        print(f"\nStatistiques:")
        print(f"  - Taille totale: {stats['total_size_mb']} MB")
        print(f"  - Statut: {stats['status']}")

    def _handle_pull_non_stream(self, response):
        """G√®re le t√©l√©chargement en mode non-stream√©"""
        result = response.json()
        return result

    def delete(self, model_name:str):
        """
            Permet de supprimer un mod√®le
        """

        try:
            payload = {"model": model_name}
            response = requests.delete(f"{self.api_url}/api/delete", json=payload)
            response.raise_for_status()
            return response.text

        except requests.exceptions.HTTPError as e:
            print("HTTP error:", e)
            if e.response is not None:
                print("Response content:", e.response.text)
            return None
        except requests.exceptions.RequestException as e:
            print("Request failed:", e)
            return None

    def version(self):
        """
            Permet de retourner la version d'ollama
        """

        try:
            response = requests.get(f"{self.api_url}/api/version")
            response.raise_for_status()
            return response.json()

        except requests.exceptions.HTTPError as e:
            print("HTTP error:", e)
            if e.response is not None:
                print("Response content:", e.response.text)
            return None
        except requests.exceptions.RequestException as e:
            print("Request failed:", e)
            return None


"""

Memo pour l'impl√©mentation des flux stream dans les autres providers

class OllamaProvider(BaseProvider):
    # ... code existant ...

    def _handle_stream(self, response):
        def stream_generator():
            for line in response.iter_lines():
                if line:
                    chunk = json.loads(line.decode('utf-8'))
                    yield chunk

                    if chunk.get('done', False):
                        break

        return OllamaStreamResult(stream_generator())


class OpenAIProvider(BaseProvider):
    # ... code existant ...

    def _handle_stream(self, response):
        def stream_generator():
            for line in response.iter_lines():
                if line:
                    line_text = line.decode('utf-8')
                    if line_text.startswith('data: '):
                        if line_text.strip() == 'data: [DONE]':
                            break
                        chunk = json.loads(line_text[6:])
                        yield chunk

        return OpenAIStreamResult(stream_generator())


class AnthropicProvider(BaseProvider):
    # ... code existant ...

    def _handle_stream(self, response):
        def stream_generator():
            for line in response.iter_lines():
                if line:
                    line_text = line.decode('utf-8')
                    if line_text.startswith('data: '):
                        chunk = json.loads(line_text[6:])
                        yield chunk

                        if chunk.get('type') == 'message_stop':
                            break

        return AnthropicStreamResult(stream_generator())


"""